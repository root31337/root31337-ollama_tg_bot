import logging
import subprocess
import asyncio
import signal
import os
from typing import Dict, Any, List, Optional, Tuple
import json
import time
from datetime import datetime
from functools import wraps
import re
import statistics
from dataclasses import dataclass
from enum import Enum
from collections import defaultdict

from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import (
    ApplicationBuilder,
    CommandHandler,
    MessageHandler,
    filters,
    CallbackQueryHandler,
    ContextTypes
)
from telegram.error import BadRequest
import ollama
from ollama import ResponseError

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
from config import CONFIG

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
def setup_logging():
    logging.basicConfig(
        level=CONFIG['LOG_LEVEL'],
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('bot.log', encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    logging.getLogger('httpx').setLevel(logging.WARNING)

logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
user_data: Dict[int, Dict] = {}
context_memory: Dict[int, Dict] = {}
rate_limit_counters: Dict[int, Tuple[int, float]] = {}
model_info_cache: Dict[str, Dict] = {}
active_requests: Dict[int, asyncio.Task] = {}
shutdown_event = asyncio.Event()
benchmark_results: Dict[str, List[Dict]] = {}
benchmark_semaphore = asyncio.Semaphore(CONFIG['BENCHMARK_MAX_PARALLEL_TASKS'])

# –ö–ª–∞—Å—Å –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö —Ç–∞–π–º–∞—É—Ç–æ–≤
class AdaptiveBenchmark:
    def __init__(self):
        self.task_times = defaultdict(list)
        self.task_timeouts = {}
    
    def get_timeout_for_task(self, task_id: str, default_timeout: int = None) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏"""
        default_timeout = default_timeout or CONFIG['BENCHMARK_DEFAULT_TIMEOUT']
        
        if task_id in self.task_times and self.task_times[task_id]:
            times = self.task_times[task_id][-5:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–π
            if times:
                avg_time = statistics.mean(times)
                adaptive_timeout = int(avg_time * 2.0) + 10  # –ó–∞–ø–∞—Å 100% + 10 —Å–µ–∫—É–Ω–¥
                result = max(default_timeout, min(adaptive_timeout, 300))  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 300 —Å–µ–∫—É–Ω–¥–∞–º–∏
                self.task_timeouts[task_id] = result
                return result
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–∞–π–º–∞—É—Ç –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
        for task_config in CONFIG['BENCHMARK_TASKS']:
            if task_config['id'] == task_id:
                return task_config.get('timeout', default_timeout)
        
        return default_timeout
    
    def record_task_time(self, task_id: str, time_taken: float):
        """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏"""
        self.task_times[task_id].append(time_taken)
        if len(self.task_times[task_id]) > 10:
            self.task_times[task_id] = self.task_times[task_id][-10:]
    
    def get_task_stats(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–∞–π–º–∞—É—Ç–∞–º"""
        stats = {}
        for task_id, times in self.task_times.items():
            if times:
                stats[task_id] = {
                    'avg_time': statistics.mean(times),
                    'max_time': max(times),
                    'min_time': min(times),
                    'count': len(times),
                    'current_timeout': self.task_timeouts.get(task_id, 'N/A')
                }
        return stats

adaptive_benchmark = AdaptiveBenchmark()

# –ö–ª–∞—Å—Å—ã –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∞
class BenchmarkTask:
    def __init__(self, id: str, name: str, prompt: str, expected_answer: str, 
                 timeout: int = None, max_tokens: int = None, weight: float = 1.0):
        self.id = id
        self.name = name
        self.prompt = prompt
        self.expected_answer = expected_answer.lower()
        self.timeout = timeout or CONFIG['BENCHMARK_DEFAULT_TIMEOUT']
        self.max_tokens = max_tokens or CONFIG['BENCHMARK_MAX_TOKENS']
        self.weight = weight
    
    def evaluate(self, response: str) -> Tuple[float, str]:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ –æ—Ç 0 –¥–æ 1"""
        response_lower = response.lower()
        
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        if self.expected_answer in response_lower:
            return 1.0, "‚úÖ –ü–æ–ª–Ω—ã–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç"
        
        # –î–ª—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á
        if self.id == 'math_problem':
            numbers = re.findall(r'\d+', response)
            if numbers:
                try:
                    last_number = int(numbers[-1])
                    if last_number == 6:
                        return 1.0, "‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —á–∏—Å–ª–æ–≤–æ–π –æ—Ç–≤–µ—Ç"
                    elif abs(last_number - 6) <= 2:
                        return 0.7, f"‚ö†Ô∏è –ë–ª–∏–∑–∫–∏–π –æ—Ç–≤–µ—Ç: {last_number}"
                    else:
                        return 0.3, f"‚ö†Ô∏è –ß–∏—Å–ª–æ –Ω–∞–π–¥–µ–Ω–æ, –Ω–æ –Ω–µ–≤–µ—Ä–Ω–æ–µ: {last_number}"
                except:
                    pass
        
        # –î–ª—è –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á
        if self.id == 'logic_puzzle':
            logic_keywords = ['–æ–¥–∏–Ω–∞–∫–æ–≤', '—Ä–∞–≤–Ω', 'same', 'equal', '–≤–µ—Å—è—Ç –æ–¥–∏–Ω–∞–∫–æ–≤–æ', '–æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –≤–µ—Å']
            if any(keyword in response_lower for keyword in logic_keywords):
                return 1.0, "‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ª–æ–≥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥"
            elif '–∂–µ–ª–µ–∑' in response_lower or '–∂–µ–ª–µ–∑–æ' in response_lower:
                return 0.5, "‚ö†Ô∏è –ß–∞—Å—Ç–∏—á–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç"
        
        # –î–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞
        if self.id == 'translation':
            translation_keywords = ['red car', 'the red car', 'red automobile', '–∫—Ä–∞—Å–Ω–∞—è –º–∞—à–∏–Ω–∞']
            if any(keyword in response_lower for keyword in translation_keywords):
                return 1.0, "‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥"
            elif 'red' in response_lower and 'car' in response_lower:
                return 0.8, "‚úÖ –û—Å–Ω–æ–≤–Ω—ã–µ —Å–ª–æ–≤–∞ –ø–µ—Ä–µ–≤–µ–¥–µ–Ω—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ"
            elif '–∫—Ä–∞—Å–Ω' in response_lower and '–º–∞—à–∏–Ω' in response_lower:
                return 0.6, "‚ö†Ô∏è –ü–µ—Ä–µ–≤–æ–¥ –Ω–µ –ø–æ–ª–Ω—ã–π"
        
        # –î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞
        if self.id == 'code_generation':
            code_patterns = ['def factorial', 'function factorial', 'factorial(n)', 'def fact']
            if any(pattern in response_lower for pattern in code_patterns):
                if 'return' in response_lower or '—Ä–µ–∫—É—Ä—Å' in response_lower:
                    return 1.0, "‚úÖ –ü–æ–ª–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ñ–∞–∫—Ç–æ—Ä–∏–∞–ª–∞"
                else:
                    return 0.8, "‚úÖ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –Ω–∞–π–¥–µ–Ω–æ"
            elif 'factorial' in response_lower:
                return 0.5, "‚ö†Ô∏è –£–ø–æ–º–∏–Ω–∞–Ω–∏–µ —Ñ–∞–∫—Ç–æ—Ä–∏–∞–ª–∞ –µ—Å—Ç—å"
        
        # –î–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        if self.id == 'summarization':
            medical_keywords = ['–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫', '–ª–µ—á–µ–Ω–∏', '–∞–Ω–∞–ª–∏–∑', '–ø—Ä–æ–≥–Ω–æ–∑', '—Ä–æ–±–æ—Ç', '—Ö–∏—Ä—É—Ä–≥', 
                               'diagnos', 'treat', 'analy', 'surger', 'robot']
            found_keywords = sum(1 for kw in medical_keywords if kw in response_lower)
            if found_keywords >= 3:
                return 1.0, "‚úÖ –û—Ç–ª–∏—á–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è"
            elif found_keywords >= 2:
                return 0.8, "‚úÖ –•–æ—Ä–æ—à–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è"
            elif found_keywords >= 1:
                return 0.5, "‚ö†Ô∏è –û—Å–Ω–æ–≤–Ω—ã–µ –∏–¥–µ–∏ —É–ø–æ–º—è–Ω—É—Ç—ã"
        
        return 0.1, "‚ùå –û—Ç–≤–µ—Ç –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–∂–∏–¥–∞–µ–º–æ–º—É"

@dataclass
class BenchmarkResult:
    model: str
    task_id: str
    task_name: str
    score: float
    response_time: float
    tokens_generated: int
    tokens_per_second: float
    evaluation: str
    raw_response: str
    timestamp: float
    timeout_used: int
    task_weight: float = 1.0
    
    def to_dict(self):
        return {
            'model': self.model,
            'task_id': self.task_id,
            'task_name': self.task_name,
            'score': self.score,
            'response_time': self.response_time,
            'tokens_generated': self.tokens_generated,
            'tokens_per_second': self.tokens_per_second,
            'evaluation': self.evaluation,
            'timestamp': self.timestamp,
            'timeout_used': self.timeout_used,
            'task_weight': self.task_weight
        }
    
    @property
    def weighted_score(self):
        return self.score * self.task_weight

class BenchmarkStatus(Enum):
    NOT_STARTED = "not_started"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"

# –î–µ–∫–æ—Ä–∞—Ç–æ—Ä—ã
def rate_limit_check(func):
    @wraps(func)
    async def wrapper(update: Update, context: ContextTypes.DEFAULT_TYPE):
        user_id = update.effective_user.id
        if check_rate_limit(user_id):
            await update.message.reply_text("‚ö†Ô∏è –í—ã –ø—Ä–µ–≤—ã—Å–∏–ª–∏ –ª–∏–º–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏–π. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ –º–∏–Ω—É—Ç—É.")
            return
        return await func(update, context)
    return wrapper

def handle_errors(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        try:
            return await func(*args, **kwargs)
        except asyncio.CancelledError:
            raise
        except BadRequest as e:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—à–∏–±–∫—É "Message is not modified"
            if "Message is not modified" in str(e):
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —ç—Ç—É –æ—à–∏–±–∫—É, –ø—Ä–æ—Å—Ç–æ –æ—Ç–≤–µ—á–∞–µ–º –Ω–∞ callback_query –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
                query = None
                for arg in args:
                    if isinstance(arg, Update) and hasattr(arg, 'callback_query'):
                        query = arg.callback_query
                        break
                
                if query:
                    try:
                        await query.answer()
                    except:
                        pass
                return
            # –î–ª—è –¥—Ä—É–≥–∏—Ö BadRequest –ª–æ–≥–∏—Ä—É–µ–º
            logger.error(f"BadRequest in {func.__name__}: {str(e)}")
        except Exception as e:
            logger.error(f"Error in {func.__name__}: {str(e)}", exc_info=True)
            
            update = None
            for arg in args:
                if isinstance(arg, Update):
                    update = arg
                    break
            
            if update:
                try:
                    if hasattr(update, 'callback_query') and update.callback_query:
                        await update.callback_query.message.reply_text("‚ö†Ô∏è –û—à–∏–±–∫–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞")
                    elif hasattr(update, 'message') and update.message:
                        await update.message.reply_text("‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞")
                except Exception as send_error:
                    logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è: {send_error}")
    return wrapper

def cancel_previous_request(func):
    @wraps(func)
    async def wrapper(update: Update, context: ContextTypes.DEFAULT_TYPE):
        user_id = update.effective_user.id
        if user_id in active_requests:
            active_requests[user_id].cancel()
            try:
                await active_requests[user_id]
            except asyncio.CancelledError:
                pass
        task = asyncio.create_task(func(update, context))
        active_requests[user_id] = task
        try:
            return await task
        finally:
            active_requests.pop(user_id, None)
    return wrapper

# –£—Ç–∏–ª–∏—Ç—ã
def setup_directories():
    """–°–æ–∑–¥–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
    os.makedirs('backups', exist_ok=True)
    os.makedirs('logs', exist_ok=True)
    os.makedirs('temp', exist_ok=True)
    os.makedirs('benchmarks', exist_ok=True)

async def backup_data():
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π"""
    try:
        timestamp = int(time.time())
        async with asyncio.Lock():
            with open(f'backups/user_data_{timestamp}.json', 'w', encoding='utf-8') as f:
                json.dump(user_data, f, ensure_ascii=False, indent=2)
            with open(f'backups/context_memory_{timestamp}.json', 'w', encoding='utf-8') as f:
                json.dump(context_memory, f, ensure_ascii=False, indent=2)
            
            backup_files = sorted([f for f in os.listdir('backups') if f.startswith('user_data_')])
            for old_file in backup_files[:-5]:
                os.remove(f'backups/{old_file}')
            
            logger.info("–†–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")

async def backup_task(context: ContextTypes.DEFAULT_TYPE):
    """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ"""
    if shutdown_event.is_set():
        return
    
    await backup_data()

async def load_backup():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏"""
    try:
        backup_files = sorted([f for f in os.listdir('backups') if f.startswith('user_data_')])
        if backup_files:
            with open(f'backups/{backup_files[-1]}', 'r', encoding='utf-8') as f:
                global user_data
                user_data = json.load(f)
        
        backup_files = sorted([f for f in os.listdir('backups') if f.startswith('context_memory_')])
        if backup_files:
            with open(f'backups/{backup_files[-1]}', 'r', encoding='utf-8') as f:
                global context_memory
                context_memory = json.load(f)
        
        logger.info("–†–µ–∑–µ—Ä–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏: {e}")

def check_rate_limit(user_id: int) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    current_time = time.time()
    count, last_time = rate_limit_counters.get(user_id, (0, current_time))
    
    if current_time - last_time > 60:
        rate_limit_counters[user_id] = (1, current_time)
        return False
    
    if count >= CONFIG['RATE_LIMIT']:
        return True
    
    rate_limit_counters[user_id] = (count + 1, last_time)
    return False

async def get_available_models(refresh: bool = False) -> List[str]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    if not refresh and model_info_cache:
        return sorted(
            list(model_info_cache.keys()),
            key=lambda x: CONFIG['MODEL_PRIORITY'].get(x.split(':')[0], 10)
        )
    
    try:
        result = await asyncio.to_thread(
            subprocess.run,
            ['ollama', 'list'],
            capture_output=True,
            text=True,
            check=True,
            timeout=10
        )
        models = [line.split()[0] for line in result.stdout.strip().split('\n')[1:]]
        
        for model in models:
            if model not in model_info_cache:
                model_info_cache[model] = {
                    'name': model, 
                    'last_used': None,
                    'usage_count': 0,
                    'benchmark_score': 0.0,
                    'benchmark_tested': False,
                    'benchmark_runs': 0,
                    'last_benchmark': None
                }
        
        return sorted(
            models,
            key=lambda x: CONFIG['MODEL_PRIORITY'].get(x.split(':')[0], 10)
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {e}")
        return []

async def get_model_info(model: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
    try:
        await asyncio.to_thread(ollama.show, model=model)
        return True
    except Exception:
        return False

def split_long_message(text: str, max_length: int = None) -> List[str]:
    """–†–∞–∑–±–∏–≤–∫–∞ –¥–ª–∏–Ω–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –Ω–∞ —á–∞—Å—Ç–∏"""
    max_length = max_length or CONFIG['MAX_MESSAGE_LENGTH']
    parts = []
    while text:
        if len(text) <= max_length:
            parts.append(text)
            break
        
        for delimiter in ['\n\n', '\n', '. ', '! ', '? ', ' ', '']:
            split_pos = text.rfind(delimiter, 0, max_length)
            if split_pos > 0:
                parts.append(text[:split_pos + len(delimiter)])
                text = text[split_pos + len(delimiter):]
                break
        else:
            parts.append(text[:max_length])
            text = text[max_length:]
    
    return parts

def trim_context(context: List[Dict], max_length: int = None) -> List[Dict]:
    """–û–±—Ä–µ–∑–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã"""
    max_length = max_length or CONFIG['MAX_CONTEXT_LENGTH']
    total_length = sum(len(msg['content']) for msg in context)
    
    while total_length > max_length and len(context) > 1:
        removed = context.pop(0)
        total_length -= len(removed['content'])
    
    return context

def initialize_user(user_id: int):
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    if user_id not in user_data:
        user_data[user_id] = {
            'current_model': CONFIG['DEFAULT_MODEL'],
            'preferences': {
                'save_context': True,
                'markdown_formatting': True,
                'notifications': True,
                'typing_indicator': True,
                'stream_responses': False
            },
            'stats': {
                'messages_sent': 0,
                'models_used': {},
                'last_active': time.time(),
                'total_tokens': 0,
                'total_requests': 0,
                'benchmarks_run': 0
            },
            'is_admin': user_id in CONFIG['ADMIN_IDS'],
            'created_at': time.time(),
            'benchmark_status': BenchmarkStatus.NOT_STARTED.value,
            'current_benchmark': None,
            'benchmark_history': []
        }
    
    if user_id not in context_memory:
        context_memory[user_id] = {}
    
    current_model = user_data[user_id]['current_model']
    if current_model not in context_memory[user_id]:
        context_memory[user_id][current_model] = []

async def send_long_message(update: Update, text: str, parse_mode: str = None):
    """–û—Ç–ø—Ä–∞–≤–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π —Å —Ä–∞–∑–±–∏–≤–∫–æ–π"""
    parts = split_long_message(text)
    for i, part in enumerate(parts):
        if i == 0:
            await update.message.reply_text(part, parse_mode=parse_mode)
        else:
            await update.effective_chat.send_message(part, parse_mode=parse_mode)

# –§—É–Ω–∫—Ü–∏–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞
class BenchmarkProgressTracker:
    def __init__(self, query, total_tasks: int):
        self.query = query
        self.total_tasks = total_tasks
        self.completed_tasks = 0
        self.start_time = time.time()
        self.last_update_time = self.start_time
        self.current_model = ""
        self.current_task = ""
    
    def increment(self, model: str = "", task: str = ""):
        self.completed_tasks += 1
        self.current_model = model
        self.current_task = task
    
    async def update_display(self):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        current_time = time.time()
        # –û–±–Ω–æ–≤–ª—è–µ–º –Ω–µ —á–∞—â–µ —á–µ–º —Ä–∞–∑ –≤ N —Å–µ–∫—É–Ω–¥
        if current_time - self.last_update_time < CONFIG['BENCHMARK_PROGRESS_UPDATE_INTERVAL']:
            return
        
        self.last_update_time = current_time
        progress_percent = (self.completed_tasks / self.total_tasks) * 100
        progress_bar = "‚ñà" * int(progress_percent / 10) + "‚ñë" * (10 - int(progress_percent / 10))
        
        elapsed_time = current_time - self.start_time
        if self.completed_tasks > 0:
            avg_time_per_task = elapsed_time / self.completed_tasks
            remaining_tasks = self.total_tasks - self.completed_tasks
            estimated_remaining = avg_time_per_task * remaining_tasks
            time_info = f"\n‚è± –í—Ä–µ–º—è: {elapsed_time:.0f}—Å | –û—Å—Ç–∞–ª–æ—Å—å: ~{estimated_remaining:.0f}—Å"
        else:
            time_info = ""
        
        try:
            await self.query.edit_message_text(
                f"üèÉ‚Äç‚ôÇÔ∏è *–ë–µ–Ω—á–º–∞—Ä–∫ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è...*\n\n"
                f"–ü—Ä–æ–≥—Ä–µ—Å—Å: {progress_bar} {progress_percent:.1f}%\n"
                f"–ó–∞–≤–µ—Ä—à–µ–Ω–æ: {self.completed_tasks}/{self.total_tasks} –∑–∞–¥–∞–Ω–∏–π{time_info}\n"
                f"–¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å: {self.current_model}\n"
                f"–¢–µ–∫—É—â–∞—è –∑–∞–¥–∞—á–∞: {self.current_task}",
                parse_mode='Markdown'
            )
        except Exception as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å: {e}")
    
    async def final_update(self):
        """–§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è"""
        elapsed_time = time.time() - self.start_time
        try:
            await self.query.edit_message_text(
                f"‚úÖ *–ë–µ–Ω—á–º–∞—Ä–∫ –∑–∞–≤–µ—Ä—à–µ–Ω!*\n\n"
                f"–í—Å–µ–≥–æ –∑–∞–¥–∞–Ω–∏–π: {self.total_tasks}\n"
                f"–û–±—â–µ–µ –≤—Ä–µ–º—è: {elapsed_time:.1f} —Å–µ–∫—É–Ω–¥\n"
                f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ –∑–∞–¥–∞—á—É: {elapsed_time/self.total_tasks:.1f}—Å\n\n"
                f"*–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...*",
                parse_mode='Markdown'
            )
        except Exception as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–¥–µ–ª–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: {e}")

async def run_benchmark_task(model: str, task: BenchmarkTask) -> Optional[BenchmarkResult]:
    """–ó–∞–ø—É—Å–∫ –æ–¥–Ω–æ–≥–æ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏"""
    async with benchmark_semaphore:
        try:
            start_time = time.time()
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ç–∞–π–º–∞—É—Ç
            timeout = adaptive_benchmark.get_timeout_for_task(task.id, task.timeout)
            
            logger.info(f"–ó–∞–ø—É—Å–∫ –∑–∞–¥–∞—á–∏ {task.name} –¥–ª—è –º–æ–¥–µ–ª–∏ {model} —Å —Ç–∞–π–º–∞—É—Ç–æ–º {timeout}—Å")
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    ollama.chat,
                    model=model,
                    messages=[{'role': 'user', 'content': task.prompt}],
                    options={
                        **CONFIG['MODEL_OPTIONS'],
                        'temperature': CONFIG['BENCHMARK_TEMPERATURE'],
                        'num_predict': task.max_tokens
                    }
                ),
                timeout=timeout
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            adaptive_benchmark.record_task_time(task.id, response_time)
            
            answer = response['message']['content']
            
            eval_count = response.get('eval_count', 0)
            tokens_per_second = eval_count / response_time if response_time > 0 else 0
            
            score, evaluation = task.evaluate(answer)
            
            result = BenchmarkResult(
                model=model,
                task_id=task.id,
                task_name=task.name,
                score=score,
                response_time=response_time,
                tokens_generated=eval_count,
                tokens_per_second=tokens_per_second,
                evaluation=evaluation,
                raw_response=answer[:200] + "..." if len(answer) > 200 else answer,
                timestamp=time.time(),
                timeout_used=timeout,
                task_weight=task.weight
            )
            
            logger.info(f"–ë–µ–Ω—á–º–∞—Ä–∫: {model} - {task.name} - –û—Ü–µ–Ω–∫–∞: {score:.2f} - –í—Ä–µ–º—è: {response_time:.2f}—Å")
            
            return result
            
        except asyncio.TimeoutError:
            logger.warning(f"–¢–∞–π–º–∞—É—Ç –±–µ–Ω—á–º–∞—Ä–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ {model} –Ω–∞ –∑–∞–¥–∞—á–µ {task.name} (—Ç–∞–π–º–∞—É—Ç: {timeout}—Å)")
            return None
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ {model}: {e}", exc_info=True)
            return None

async def run_full_benchmark(models: List[str], user_id: int, query):
    """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞ –¥–ª—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π"""
    user_data[user_id]['benchmark_status'] = BenchmarkStatus.RUNNING.value
    
    # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏
    tasks = []
    for task_config in CONFIG['BENCHMARK_TASKS']:
        task = BenchmarkTask(
            id=task_config['id'],
            name=task_config['name'],
            prompt=task_config['prompt'],
            expected_answer=task_config['expected_answer'],
            timeout=task_config.get('timeout', CONFIG['BENCHMARK_DEFAULT_TIMEOUT']),
            max_tokens=task_config.get('max_tokens', CONFIG['BENCHMARK_MAX_TOKENS']),
            weight=task_config.get('weight', 1.0)
        )
        tasks.append(task)
    
    results = {}
    total_tasks = len(models) * len(tasks)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–∫–µ—Ä –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    progress_tracker = BenchmarkProgressTracker(query, total_tasks)
    await progress_tracker.update_display()
    
    for model in models:
        model_results = []
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
        if not await get_model_info(model):
            logger.warning(f"–ú–æ–¥–µ–ª—å {model} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∞")
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏
            for _ in tasks:
                progress_tracker.increment(model, "–ü—Ä–æ–ø—É—â–µ–Ω–∞ (–Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)")
                await progress_tracker.update_display()
            continue
        
        for task in tasks:
            if shutdown_event.is_set():
                break
            
            progress_tracker.increment(model, task.name)
            
            result = await run_benchmark_task(model, task)
            if result:
                model_results.append(result)
                
                if model not in benchmark_results:
                    benchmark_results[model] = []
                benchmark_results[model].append(result.to_dict())
            
            await progress_tracker.update_display()
        
        if model_results:
            # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞
            total_weight = sum(r.task_weight for r in model_results)
            weighted_sum = sum(r.score * r.task_weight for r in model_results)
            avg_score = weighted_sum / total_weight if total_weight > 0 else 0
            
            avg_time = statistics.mean([r.response_time for r in model_results])
            avg_tps = statistics.mean([r.tokens_per_second for r in model_results])
            
            results[model] = {
                'avg_score': avg_score,
                'avg_time': avg_time,
                'avg_tps': avg_tps,
                'weighted_score': avg_score,
                'results': model_results
            }
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
            if model in model_info_cache:
                model_info_cache[model]['benchmark_score'] = avg_score
                model_info_cache[model]['benchmark_tested'] = True
                model_info_cache[model]['benchmark_runs'] = model_info_cache[model].get('benchmark_runs', 0) + 1
                model_info_cache[model]['last_benchmark'] = time.time()
    
    # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    await progress_tracker.final_update()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if results:
        await save_benchmark_results(results, user_id)
    
    user_data[user_id]['benchmark_status'] = BenchmarkStatus.COMPLETED.value
    user_data[user_id]['stats']['benchmarks_run'] += 1
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –±–µ–Ω—á–º–∞—Ä–∫–∞
    benchmark_history_entry = {
        'timestamp': time.time(),
        'models_tested': list(results.keys()),
        'total_tasks': total_tasks,
        'results_summary': {model: data['avg_score'] for model, data in results.items()}
    }
    user_data[user_id]['benchmark_history'].append(benchmark_history_entry)
    
    return results

async def save_benchmark_results(results: Dict, user_id: int):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–∞ –≤ —Ñ–∞–π–ª"""
    try:
        timestamp = int(time.time())
        filename = f'benchmarks/benchmark_{user_id}_{timestamp}.json'
        
        data = {
            'timestamp': timestamp,
            'user_id': user_id,
            'results': {},
            'summary': {},
            'adaptive_timeouts': adaptive_benchmark.task_times,
            'task_stats': adaptive_benchmark.get_task_stats()
        }
        
        for model, model_data in results.items():
            data['results'][model] = {
                'avg_score': model_data['avg_score'],
                'weighted_score': model_data['weighted_score'],
                'avg_time': model_data['avg_time'],
                'avg_tps': model_data['avg_tps'],
                'task_results': [r.to_dict() for r in model_data['results']]
            }
        
        # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–∫—É
        sorted_models = sorted(results.items(), key=lambda x: x[1]['weighted_score'], reverse=True)
        data['summary'] = {
            'top_model': sorted_models[0][0] if sorted_models else None,
            'ranking': [{'model': m, 'score': d['weighted_score'], 'raw_score': d['avg_score']} 
                       for m, d in sorted_models],
            'total_models': len(results),
            'timestamp': timestamp
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
        return filename
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–∞: {e}")
        return None

def format_benchmark_results(results: Dict) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    if not results:
        return "‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è"
    
    lines = ["üìä *–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞ –º–æ–¥–µ–ª–µ–π*\n"]
    
    sorted_models = sorted(results.items(), key=lambda x: x[1]['weighted_score'], reverse=True)
    
    for i, (model, data) in enumerate(sorted_models):
        score = data['weighted_score']
        score_stars = "‚≠ê" * int(score * 5)
        
        lines.append(f"\n*{i+1}. {model}*")
        lines.append(f"   –û—Ü–µ–Ω–∫–∞: *{score:.2f}/1.0* {score_stars}")
        lines.append(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {data['avg_time']:.2f}—Å")
        lines.append(f"   –¢–æ–∫–µ–Ω–æ–≤/—Å–µ–∫: {data['avg_tps']:.1f}")
        
        if 'raw_score' in data:
            lines.append(f"   –°—ã—Ä–∞—è –æ—Ü–µ–Ω–∫–∞: {data['raw_score']:.2f}")
        
        lines.append(f"   *–î–µ—Ç–∞–ª–∏ –ø–æ –∑–∞–¥–∞—á–∞–º:*")
        for task_result in data['results']:
            status = "‚úÖ" if task_result.score >= 0.7 else "‚ö†Ô∏è" if task_result.score >= 0.3 else "‚ùå"
            weight_marker = f" ‚öñÔ∏è{task_result.task_weight}" if task_result.task_weight != 1.0 else ""
            timeout_info = f" (‚è±{task_result.timeout_used}—Å)" if hasattr(task_result, 'timeout_used') else ""
            
            lines.append(f"   {status}{weight_marker} {task_result.task_name}:")
            lines.append(f"     - –û—Ü–µ–Ω–∫–∞: {task_result.score:.1f}{timeout_info}")
            lines.append(f"     - –í—Ä–µ–º—è: {task_result.response_time:.1f}—Å")
            lines.append(f"     - –í–µ—Ä–¥–∏–∫—Ç: {task_result.evaluation}")
    
    # –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
    lines.append("\n*üìà –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞:*")
    lines.append("```")
    lines.append(f"{'–ú–æ–¥–µ–ª—å':<20} {'–û—Ü–µ–Ω–∫–∞':<8} {'–í—Ä–µ–º—è':<8} {'T/s':<8}")
    lines.append("-" * 50)
    for model, data in sorted_models:
        lines.append(f"{model:<20} {data['weighted_score']:.2f}     {data['avg_time']:.2f}—Å    {data['avg_tps']:.1f}")
    lines.append("```")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–∞–π–º–∞—É—Ç–æ–≤
    timeout_stats = adaptive_benchmark.get_task_stats()
    if timeout_stats:
        lines.append("\n*‚è± –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–∞–π–º–∞—É—Ç–æ–≤:*")
        for task_id, stats in timeout_stats.items():
            task_name = next((t['name'] for t in CONFIG['BENCHMARK_TASKS'] if t['id'] == task_id), task_id)
            lines.append(f"  ‚Ä¢ {task_name}: —Å—Ä. {stats['avg_time']:.1f}—Å, —Ç–∞–π–º–∞—É—Ç {stats['current_timeout']}—Å")
    
    return "\n".join(lines)

async def get_model_leaderboard(min_tests: int = 0) -> str:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –ª–∏–¥–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π"""
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ—Ö–æ–¥–∏–ª–∏ –±–µ–Ω—á–º–∞—Ä–∫
    tested_models = {}
    
    # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º benchmark_results
    for model, results in benchmark_results.items():
        if results:
            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            latest_result = max(results, key=lambda x: x.get('timestamp', 0))
            score = latest_result.get('score', 0)
            tests = len(results)
            
            if tests >= min_tests:
                tested_models[model] = {
                    'benchmark_score': score,
                    'benchmark_tested': True,
                    'benchmark_runs': tests,
                    'last_benchmark': latest_result.get('timestamp'),
                    'usage_count': model_info_cache.get(model, {}).get('usage_count', 0)
                }
    
    # –ó–∞—Ç–µ–º –¥–æ–±–∞–≤–ª—è–µ–º –∏–∑ model_info_cache
    for model, info in model_info_cache.items():
        if info.get('benchmark_tested', False):
            if model not in tested_models:
                score = info.get('benchmark_score', 0)
                tests = info.get('benchmark_runs', 1)
                
                if tests >= min_tests:
                    tested_models[model] = {
                        'benchmark_score': score,
                        'benchmark_tested': True,
                        'benchmark_runs': tests,
                        'last_benchmark': info.get('last_benchmark'),
                        'usage_count': info.get('usage_count', 0)
                    }
    
    if not tested_models:
        return "üìä *–¢–∞–±–ª–∏—Ü–∞ –ª–∏–¥–µ—Ä–æ–≤*\n\n–ü–æ–∫–∞ –Ω–µ—Ç –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –±–µ–Ω—á–º–∞—Ä–∫ –∫–æ–º–∞–Ω–¥–æ–π /benchmark!"
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏ –ø–æ –æ—Ü–µ–Ω–∫–µ
    sorted_models = sorted(
        tested_models.items(),
        key=lambda x: x[1].get('benchmark_score', 0),
        reverse=True
    )
    
    lines = ["üèÜ *–¢–∞–±–ª–∏—Ü–∞ –ª–∏–¥–µ—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π*\n"]
    
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_tests = sum(info.get('benchmark_runs', 0) for _, info in tested_models.items())
    avg_score = statistics.mean([info.get('benchmark_score', 0) for _, info in tested_models.items()])
    lines.append(f"–í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π: {len(tested_models)} | –í—Å–µ–≥–æ —Ç–µ—Å—Ç–æ–≤: {total_tests} | –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: {avg_score:.2f}\n")
    
    lines.append("```")
    lines.append(f"{'–ú–µ—Å—Ç–æ':<6} {'–ú–æ–¥–µ–ª—å':<25} {'–û—Ü–µ–Ω–∫–∞':<8} {'–¢–µ—Å—Ç–æ–≤':<8} {'–ò—Å–ø.':<6} {'–ü–æ—Å–ª–µ–¥–Ω–∏–π':<10}")
    lines.append("-" * 70)
    
    for i, (model, data) in enumerate(sorted_models[:15]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-15
        score = data.get('benchmark_score', 0)
        tests = data.get('benchmark_runs', 0)
        uses = data.get('usage_count', 0)
        last_test = data.get('last_benchmark')
        
        last_str = ""
        if last_test:
            last_dt = datetime.fromtimestamp(last_test)
            last_str = last_dt.strftime('%d.%m %H:%M')
        
        medal = ""
        if i == 0:
            medal = "ü•á "
        elif i == 1:
            medal = "ü•à "
        elif i == 2:
            medal = "ü•â "
        elif i < 10:
            medal = f"{i+1} "
        
        score_str = f"{score:.2f}"
        tests_str = f"{tests}"
        uses_str = f"{uses}"
        
        lines.append(f"{medal:<4} {model:<25} {score_str:<8} {tests_str:<8} {uses_str:<6} {last_str:<10}")
    
    lines.append("```")
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    if len(sorted_models) > 15:
        lines.append(f"\n... –∏ –µ—â–µ {len(sorted_models) - 15} –º–æ–¥–µ–ª–µ–π")
    
    # –õ—É—á—à–∞—è –∏ —Ö—É–¥—à–∞—è –º–æ–¥–µ–ª—å
    if len(sorted_models) >= 2:
        best_model, best_data = sorted_models[0]
        worst_model, worst_data = sorted_models[-1]
        lines.append(f"\nüéØ *–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å:* {best_model} ({best_data.get('benchmark_score', 0):.2f})")
        lines.append(f"üìâ *–•—É–¥—à–∞—è –º–æ–¥–µ–ª—å:* {worst_model} ({worst_data.get('benchmark_score', 0):.2f})")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–µ—Å—Ç–∞–º
    task_stats = adaptive_benchmark.get_task_stats()
    if task_stats:
        lines.append(f"\nüìã *–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–¥–∞—á:*")
        for task_id, stats in task_stats.items():
            task_name = next((t['name'] for t in CONFIG['BENCHMARK_TASKS'] if t['id'] == task_id), task_id)
            lines.append(f"  ‚Ä¢ {task_name}: {stats['count']} —Ç–µ—Å—Ç–æ–≤, —Å—Ä. –≤—Ä–µ–º—è {stats['avg_time']:.1f}—Å")
    
    return "\n".join(lines)

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∫–æ–º–∞–Ω–¥
@handle_errors
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start"""
    user_id = update.effective_user.id
    initialize_user(user_id)
    
    status = "‚úÖ –í–∫–ª" if user_data[user_id]['preferences']['save_context'] else "‚ùå –í—ã–∫–ª"
    stream_status = "‚úÖ –í–∫–ª" if user_data[user_id]['preferences']['stream_responses'] else "‚ùå –í—ã–∫–ª"
    
    keyboard = [
        [InlineKeyboardButton("üìù –í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å", callback_data='show_models')],
        [InlineKeyboardButton("üÜï –ù–æ–≤—ã–π –¥–∏–∞–ª–æ–≥", callback_data='new_chat')],
        [InlineKeyboardButton(f"üß† –ö–æ–Ω—Ç–µ–∫—Å—Ç: {status}", callback_data='toggle_context')],
        [InlineKeyboardButton(f"üåÄ –ü–æ—Ç–æ–∫: {stream_status}", callback_data='toggle_stream')],
        [InlineKeyboardButton("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏", callback_data='settings')],
        [InlineKeyboardButton("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", callback_data='show_stats')]
    ]
    
    if user_data[user_id]['is_admin']:
        keyboard.append([InlineKeyboardButton("üõ† –ê–¥–º–∏–Ω-–ø–∞–Ω–µ–ª—å", callback_data='admin_panel')])
        keyboard.append([InlineKeyboardButton("üèÜ –ë–µ–Ω—á–º–∞—Ä–∫", callback_data='benchmark_menu')])
    
    await update.message.reply_text(
        "ü§ñ *–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ Ollama Telegram Bot!*\n\n"
        "–Ø –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é –¥–æ—Å—Ç—É–ø –∫ —Ä–∞–∑–ª–∏—á–Ω—ã–º —è–∑—ã–∫–æ–≤—ã–º –º–æ–¥–µ–ª—è–º —á–µ—Ä–µ–∑ Ollama.",
        reply_markup=InlineKeyboardMarkup(keyboard),
        parse_mode='Markdown'
    )

@handle_errors
@rate_limit_check
async def benchmark_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞"""
    user_id = update.effective_user.id
    initialize_user(user_id)
    
    if not user_data[user_id]['is_admin']:
        await update.message.reply_text("üö´ –≠—Ç–∞ –∫–æ–º–∞–Ω–¥–∞ –¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞–º")
        return
    
    await show_benchmark_menu(update.callback_query if update.callback_query else None)

@handle_errors
async def leaderboard_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã –ª–∏–¥–µ—Ä–æ–≤"""
    user_id = update.effective_user.id
    initialize_user(user_id)
    
    # –ú–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤
    min_tests = 0
    if context.args and context.args[0].isdigit():
        min_tests = int(context.args[0])
    
    leaderboard = await get_model_leaderboard(min_tests)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º—è –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
    current_time = datetime.now().strftime('%H:%M:%S')
    leaderboard_text = f"{leaderboard}\n\n_‚è± –û–±–Ω–æ–≤–ª–µ–Ω–æ –≤ {current_time}_"
    
    await update.message.reply_text(leaderboard_text, parse_mode='Markdown')

@handle_errors
async def stats_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞"""
    user_id = update.effective_user.id
    initialize_user(user_id)
    
    total_benchmarks = sum(u['stats']['benchmarks_run'] for u in user_data.values())
    total_tested_models = len([m for m, info in model_info_cache.items() if info.get('benchmark_tested', False)])
    
    task_stats = adaptive_benchmark.get_task_stats()
    total_task_runs = sum(stats['count'] for stats in task_stats.values()) if task_stats else 0
    
    stats_text = (
        f"üìà *–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞*\n\n"
        f"‚Ä¢ –í—Å–µ–≥–æ –∑–∞–ø—É—Å–∫–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–∞: {total_benchmarks}\n"
        f"‚Ä¢ –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ –º–æ–¥–µ–ª–µ–π: {total_tested_models}\n"
        f"‚Ä¢ –í—Å–µ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ –∑–∞–¥–∞—á: {total_task_runs}\n"
        f"‚Ä¢ –ê–∫—Ç–∏–≤–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤: {sum(1 for u in user_data.values() if u['benchmark_status'] == 'running')}\n"
    )
    
    if task_stats:
        stats_text += f"\n*–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–¥–∞—á–∞–º:*\n"
        for task_id, stats in task_stats.items():
            task_name = next((t['name'] for t in CONFIG['BENCHMARK_TASKS'] if t['id'] == task_id), task_id)
            stats_text += f"‚Ä¢ {task_name}: {stats['count']} —Ç–µ—Å—Ç–æ–≤, —Å—Ä. {stats['avg_time']:.1f}—Å\n"
    
    await update.message.reply_text(stats_text, parse_mode='Markdown')

@handle_errors
@rate_limit_check
@cancel_previous_request
async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
    user_id = update.effective_user.id
    initialize_user(user_id)
    current_model = user_data[user_id]['current_model']
    message_text = update.message.text
    
    if CONFIG['MODEL_HEALTH_CHECK'] and not await get_model_info(current_model):
        await update.message.reply_text(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {current_model} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å.")
        return
    
    user_data[user_id]['stats']['messages_sent'] += 1
    user_data[user_id]['stats']['last_active'] = time.time()
    user_data[user_id]['stats']['total_requests'] += 1
    
    if current_model not in user_data[user_id]['stats']['models_used']:
        user_data[user_id]['stats']['models_used'][current_model] = 0
    user_data[user_id]['stats']['models_used'][current_model] += 1
    
    if user_data[user_id]['preferences']['save_context']:
        if current_model not in context_memory[user_id]:
            context_memory[user_id][current_model] = []
        
        context_memory[user_id][current_model].append({'role': 'user', 'content': message_text})
        context_memory[user_id][current_model] = trim_context(context_memory[user_id][current_model])
    
    if user_data[user_id]['preferences']['typing_indicator']:
        await asyncio.sleep(CONFIG['TYPING_DELAY'])
        await context.bot.send_chat_action(chat_id=update.effective_chat.id, action='typing')
    
    notification = None
    if user_data[user_id]['preferences']['notifications']:
        notification = await update.message.reply_text("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...")
    
    try:
        messages = context_memory[user_id][current_model] if user_data[user_id]['preferences']['save_context'] else []
        messages.append({'role': 'user', 'content': message_text})
        
        if user_data[user_id]['preferences']['markdown_formatting']:
            header = (
                f"*–ú–æ–¥–µ–ª—å:* {current_model}\n"
                f"*–ó–∞–ø—Ä–æ—Å:* `{message_text[:100]}{'...' if len(message_text) > 100 else ''}`\n\n"
                f"*–û—Ç–≤–µ—Ç:*\n"
            )
            parse_mode = 'Markdown'
        else:
            header = (
                f"–ú–æ–¥–µ–ª—å: {current_model}\n"
                f"–ó–∞–ø—Ä–æ—Å: {message_text[:100]}{'...' if len(message_text) > 100 else ''}\n\n"
                f"–û—Ç–≤–µ—Ç:\n"
            )
            parse_mode = None
        
        if user_data[user_id]['preferences']['stream_responses']:
            full_response = ""
            message = await update.message.reply_text(header + "‚åõ –ù–∞—á–∏–Ω–∞—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é...", parse_mode=parse_mode)
            
            async def stream_response():
                nonlocal full_response
                response = await asyncio.to_thread(
                    ollama.chat,
                    model=current_model,
                    messages=messages,
                    options=CONFIG['MODEL_OPTIONS'],
                    stream=True
                )
                
                for chunk in response:
                    if shutdown_event.is_set():
                        raise asyncio.CancelledError()
                    
                    content = chunk['message']['content']
                    full_response += content
                    
                    if len(full_response) % 50 == 0:
                        try:
                            await message.edit_text(header + full_response, parse_mode=parse_mode)
                        except:
                            pass
                
                return full_response
            
            try:
                answer = await asyncio.wait_for(
                    stream_response(),
                    timeout=CONFIG['MODEL_TIMEOUT']
                )
                await message.edit_text(header + answer, parse_mode=parse_mode)
                
                if 'eval_count' in response:
                    user_data[user_id]['stats']['total_tokens'] += response['eval_count']
                
                if user_data[user_id]['preferences']['save_context']:
                    context_memory[user_id][current_model].append({'role': 'assistant', 'content': answer})
                
            except asyncio.TimeoutError:
                await message.edit_text(header + full_response + "\n\n‚ö†Ô∏è –î–æ—Å—Ç–∏–≥–Ω—É—Ç —Ç–∞–π–º–∞—É—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏", parse_mode=parse_mode)
                raise
                
        else:
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    ollama.chat,
                    model=current_model,
                    messages=messages,
                    options=CONFIG['MODEL_OPTIONS']
                ),
                timeout=CONFIG['MODEL_TIMEOUT']
            )
            
            answer = response['message']['content']
            message_parts = split_long_message(answer, CONFIG['MAX_MESSAGE_LENGTH'] - len(header))
            
            first_part = header + message_parts[0]
            if notification:
                try:
                    await notification.edit_text(first_part, parse_mode=parse_mode)
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏—è: {e}")
                    await update.message.reply_text(first_part, parse_mode=parse_mode)
            else:
                await update.message.reply_text(first_part, parse_mode=parse_mode)
            
            for part in message_parts[1:]:
                await update.effective_chat.send_message(part, parse_mode=parse_mode)
            
            if 'eval_count' in response:
                user_data[user_id]['stats']['total_tokens'] += response['eval_count']
            
            if user_data[user_id]['preferences']['save_context']:
                context_memory[user_id][current_model].append({'role': 'assistant', 'content': answer})
        
        model_info_cache[current_model]['last_used'] = time.time()
        model_info_cache[current_model]['usage_count'] = model_info_cache[current_model].get('usage_count', 0) + 1
    
    except asyncio.TimeoutError:
        error_msg = (
            "‚è≥ –ú–æ–¥–µ–ª—å –¥–æ–ª–≥–æ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:\n"
            "1. –°–æ–∫—Ä–∞—Ç–∏—Ç—å –∑–∞–ø—Ä–æ—Å\n"
            "2. –û—Ç–ø—Ä–∞–≤–∏—Ç—å –µ–≥–æ —Å–Ω–æ–≤–∞\n"
            "3. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å (/model)\n"
            "4. –û—Ç–∫–ª—é—á–∏—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (/settings)"
        )
        logger.warning(f"–¢–∞–π–º–∞—É—Ç –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id} —Å –º–æ–¥–µ–ª—å—é {current_model}")
        if notification:
            await notification.edit_text(error_msg)
        else:
            await update.message.reply_text(error_msg)
    
    except ResponseError as e:
        if "Message too long" in str(e):
            error_msg = "‚ö†Ô∏è –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å."
        else:
            error_msg = f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏: {str(e)}"
        
        logger.error(f"–û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {error_msg}")
        if notification:
            await notification.edit_text(error_msg)
        else:
            await update.message.reply_text(error_msg)
    
    except Exception as e:
        error_msg = "‚ö†Ô∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞"
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {e}", exc_info=True)
        if notification:
            await notification.edit_text(error_msg)
        else:
            await update.message.reply_text(error_msg)

@handle_errors
async def button_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ inline-–∫–Ω–æ–ø–æ–∫"""
    query = update.callback_query
    
    try:
        await query.answer()
    except Exception as e:
        if "Query is too old" in str(e):
            logger.warning(f"–ü—Ä–æ—Å—Ä–æ—á–µ–Ω–Ω—ã–π callback: {query.data}")
            return
        raise
    
    user_id = query.from_user.id
    initialize_user(user_id)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–Ω–æ–ø–æ–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞
    if query.data == 'benchmark_menu':
        await show_benchmark_menu(query)
    
    elif query.data == 'run_full_benchmark':
        await run_full_benchmark_handler(update, context)
    
    elif query.data == 'show_leaderboard':
        await show_leaderboard_handler(update, context)
    
    elif query.data == 'show_benchmark_tasks':
        await show_benchmark_tasks_handler(update, context)
    
    elif query.data == 'test_single_model':
        await test_single_model_handler(update, context)
    
    elif query.data.startswith('benchmark_model_'):
        await benchmark_model_handler(update, context)
    
    elif query.data == 'benchmark_stats':
        await benchmark_stats_handler(update, context)
    
    elif query.data == 'clear_benchmark_cache':
        await clear_benchmark_cache_handler(update, context)
    
    # ... –æ—Å—Ç–∞–ª—å–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∫–Ω–æ–ø–æ–∫
    elif query.data == 'toggle_context':
        user_data[user_id]['preferences']['save_context'] = not user_data[user_id]['preferences']['save_context']
        status = "‚úÖ –í–∫–ª" if user_data[user_id]['preferences']['save_context'] else "‚ùå –í—ã–∫–ª"
        await show_main_menu(query, f"üß† –ö–æ–Ω—Ç–µ–∫—Å—Ç: {status}")
    
    elif query.data == 'toggle_stream':
        user_data[user_id]['preferences']['stream_responses'] = not user_data[user_id]['preferences']['stream_responses']
        status = "‚úÖ –í–∫–ª" if user_data[user_id]['preferences']['stream_responses'] else "‚ùå –í—ã–∫–ª"
        await show_main_menu(query, f"üåÄ –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {status}")
    
    elif query.data == 'show_models':
        await show_models_menu(query)
    
    elif query.data == 'refresh_models':
        await query.edit_message_text("üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π...")
        await show_models_menu(query, refresh=True)
    
    elif query.data == 'new_chat':
        context_memory[user_id] = {}
        await show_main_menu(query, "üÜï –ù–æ–≤—ã–π –¥–∏–∞–ª–æ–≥ –Ω–∞—á–∞—Ç. –ö–æ–Ω—Ç–µ–∫—Å—Ç –æ—á–∏—â–µ–Ω.")
    
    elif query.data.startswith('model_'):
        model_name = query.data[6:]
        if CONFIG['MODEL_HEALTH_CHECK'] and not await get_model_info(model_name):
            await query.answer(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model_name} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞", show_alert=True)
            return
        
        user_data[user_id]['current_model'] = model_name
        model_info_cache[model_name]['last_used'] = time.time()
        model_info_cache[model_name]['usage_count'] = model_info_cache[model_name].get('usage_count', 0) + 1
        await show_main_menu(query, f"‚úÖ –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {model_name}")
    
    elif query.data == 'settings':
        await show_settings_menu(query)
    
    elif query.data == 'toggle_markdown':
        user_data[user_id]['preferences']['markdown_formatting'] = not user_data[user_id]['preferences']['markdown_formatting']
        await show_settings_menu(query)
    
    elif query.data == 'toggle_notifications':
        user_data[user_id]['preferences']['notifications'] = not user_data[user_id]['preferences']['notifications']
        await show_settings_menu(query)
    
    elif query.data == 'toggle_typing':
        user_data[user_id]['preferences']['typing_indicator'] = not user_data[user_id]['preferences']['typing_indicator']
        await show_settings_menu(query)
    
    elif query.data == 'show_stats':
        await show_stats_menu(query)
    
    elif query.data == 'admin_panel':
        if user_data[user_id]['is_admin']:
            await show_admin_panel(query)
        else:
            await query.answer("üö´ –£ –≤–∞—Å –Ω–µ—Ç –ø—Ä–∞–≤ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞", show_alert=True)
    
    elif query.data == 'back_to_menu':
        await show_main_menu(query)
    
    elif query.data == 'force_backup':
        if user_data[user_id]['is_admin']:
            await query.answer("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –±—ç–∫–∞–ø–∞...", show_alert=False)
            await backup_data()
            await query.answer("‚úÖ –ë—ç–∫–∞–ø —Å–æ–∑–¥–∞–Ω", show_alert=False)
        else:
            await query.answer("üö´ –£ –≤–∞—Å –Ω–µ—Ç –ø—Ä–∞–≤ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞", show_alert=True)

@handle_errors
async def show_main_menu(query, text: str = None):
    """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≥–ª–∞–≤–Ω–æ–≥–æ –º–µ–Ω—é"""
    user_id = query.from_user.id
    status = "‚úÖ –í–∫–ª" if user_data[user_id]['preferences']['save_context'] else "‚ùå –í—ã–∫–ª"
    stream_status = "‚úÖ –í–∫–ª" if user_data[user_id]['preferences']['stream_responses'] else "‚ùå –í—ã–∫–ª"
    
    keyboard = [
        [InlineKeyboardButton("üìù –í—ã–±—Ä–∞—Ç—å –º–æ–¥–µ–ª—å", callback_data='show_models')],
        [InlineKeyboardButton("üÜï –ù–æ–≤—ã–π –¥–∏–∞–ª–æ–≥", callback_data='new_chat')],
        [InlineKeyboardButton(f"üß† –ö–æ–Ω—Ç–µ–∫—Å—Ç: {status}", callback_data='toggle_context')],
        [InlineKeyboardButton(f"üåÄ –ü–æ—Ç–æ–∫: {stream_status}", callback_data='toggle_stream')],
        [InlineKeyboardButton("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏", callback_data='settings')],
        [InlineKeyboardButton("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", callback_data='show_stats')]
    ]
    
    if user_data[user_id]['is_admin']:
        keyboard.append([InlineKeyboardButton("üõ† –ê–¥–º–∏–Ω-–ø–∞–Ω–µ–ª—å", callback_data='admin_panel')])
        keyboard.append([InlineKeyboardButton("üèÜ –ë–µ–Ω—á–º–∞—Ä–∫", callback_data='benchmark_menu')])
    
    try:
        await query.edit_message_text(
            text=text or "–ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é",
            reply_markup=InlineKeyboardMarkup(keyboard),
            parse_mode='Markdown'
        )
    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –º–µ–Ω—é: {e}")

@handle_errors
async def show_models_menu(query, refresh: bool = False):
    """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –º–µ–Ω—é –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–µ–π"""
    user_id = query.from_user.id
    models = await get_available_models(refresh=refresh)
    
    if not models:
        await query.edit_message_text(
            "‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Ollama –∑–∞–ø—É—â–µ–Ω.",
            reply_markup=InlineKeyboardMarkup([
                [InlineKeyboardButton("üîÑ –û–±–Ω–æ–≤–∏—Ç—å", callback_data='refresh_models')],
                [InlineKeyboardButton("üîô –ù–∞–∑–∞–¥", callback_data='back_to_menu')]
            ])
        )
        return
    
    buttons = []
    for model in models:
        model_info = model_info_cache.get(model, {})
        last_used = model_info.get('last_used')
        usage_count = model_info.get('usage_count', 0)
        benchmark_score = model_info.get('benchmark_score', 0)
        
        score_text = f" ‚≠ê{benchmark_score:.1f}" if benchmark_score > 0 else ""
        
        if last_used:
            last_used_str = datetime.fromtimestamp(last_used).strftime('%d.%m')
            label = f"{model}{score_text} ({last_used_str}, {usage_count}x)"
        else:
            label = f"{model}{score_text} (–Ω–æ–≤—ã–π)"
        
        buttons.append(InlineKeyboardButton(label, callback_data=f"model_{model}"))
    
    keyboard = [buttons[i:i + 2] for i in range(0, len(buttons), 2)]
    keyboard.append([InlineKeyboardButton("üîÑ –û–±–Ω–æ–≤–∏—Ç—å —Å–ø–∏—Å–æ–∫", callback_data='refresh_models')])
    keyboard.append([InlineKeyboardButton("üîô –ù–∞–∑–∞–¥", callback_data='back_to_menu')])
    
    await query.edit_message_text(
        "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å (‚≠ê–æ—Ü–µ–Ω–∫–∞, –¥–∞—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π):",
        reply_markup=InlineKeyboardMarkup(keyboard)
    )

@handle_errors
async def show_settings_menu(query):
    """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –º–µ–Ω—é –Ω–∞—Å—Ç—Ä–æ–µ–∫"""
    user_id = query.from_user.id
    prefs = user_data[user_id]['preferences']
    
    settings_text = (
        "‚öôÔ∏è *–ù–∞—Å—Ç—Ä–æ–π–∫–∏*\n\n"
        f"‚Ä¢ üß† –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {'‚úÖ –í–∫–ª' if prefs['save_context'] else '‚ùå –í—ã–∫–ª'}\n"
        f"‚Ä¢ üìù Markdown —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: {'‚úÖ –í–∫–ª' if prefs['markdown_formatting'] else '‚ùå –í—ã–∫–ª'}\n"
        f"‚Ä¢ üîî –£–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: {'‚úÖ –í–∫–ª' if prefs['notifications'] else '‚ùå –í—ã–∫–ª'}\n"
        f"‚Ä¢ ‚úçÔ∏è –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –Ω–∞–±–æ—Ä–∞: {'‚úÖ –í–∫–ª' if prefs['typing_indicator'] else '‚ùå –í—ã–∫–ª'}\n"
        f"‚Ä¢ üåÄ –ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è: {'‚úÖ –í–∫–ª' if prefs['stream_responses'] else '‚ùå –í—ã–∫–ª'}"
    )
    
    keyboard = [
        [InlineKeyboardButton("–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç", callback_data='toggle_context')],
        [InlineKeyboardButton("–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å Markdown", callback_data='toggle_markdown')],
        [InlineKeyboardButton("–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è", callback_data='toggle_notifications')],
        [InlineKeyboardButton("–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä", callback_data='toggle_typing')],
        [InlineKeyboardButton("–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –ø–æ—Ç–æ–∫", callback_data='toggle_stream')],
        [InlineKeyboardButton("üîô –ù–∞–∑–∞–¥", callback_data='back_to_menu')]
    ]
    
    await query.edit_message_text(
        settings_text,
        reply_markup=InlineKeyboardMarkup(keyboard),
        parse_mode='Markdown'
    )

@handle_errors
async def show_stats_menu(query):
    """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
    user_id = query.from_user.id
    stats = user_data[user_id]['stats']
    
    models_used = "\n".join(
        f"‚Ä¢ {model}: {count} —Å–æ–æ–±—â." 
        for model, count in sorted(
            stats['models_used'].items(),
            key=lambda x: x[1],
            reverse=True
        )[:5]
    )
    
    stats_text = (
        f"üìä *–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞*\n\n"
        f"‚Ä¢ –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π: {stats['messages_sent']}\n"
        f"‚Ä¢ –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {stats['total_requests']}\n"
        f"‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏:\n{models_used}\n"
        f"‚Ä¢ –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {stats.get('total_tokens', 0)}\n"
        f"‚Ä¢ –ü–æ—Å–ª–µ–¥–Ω—è—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {datetime.fromtimestamp(stats['last_active']).strftime('%d.%m %H:%M')}\n"
        f"‚Ä¢ –¢–µ–∫—É—â–∞—è –º–æ–¥–µ–ª—å: {user_data[user_id]['current_model']}\n"
        f"‚Ä¢ –ê–∫—Ç–∏–≤–µ–Ω —Å: {datetime.fromtimestamp(user_data[user_id]['created_at']).strftime('%d.%m.%Y')}"
    )
    
    if user_data[user_id]['is_admin']:
        stats_text += f"\n‚Ä¢ –ó–∞–ø—É—Å–∫–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–∞: {stats.get('benchmarks_run', 0)}"
    
    await query.edit_message_text(
        stats_text,
        reply_markup=InlineKeyboardMarkup([
            [InlineKeyboardButton("üîô –ù–∞–∑–∞–¥", callback_data='back_to_menu')]
        ]),
        parse_mode='Markdown'
    )

@handle_errors
async def show_admin_panel(query):
    """–ê–¥–º–∏–Ω-–ø–∞–Ω–µ–ª—å"""
    user_id = query.from_user.id
    if not user_data[user_id]['is_admin']:
        await query.answer("üö´ –£ –≤–∞—Å –Ω–µ—Ç –ø—Ä–∞–≤ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞", show_alert=True)
        return
    
    total_users = len(user_data)
    active_users = sum(1 for uid, data in user_data.items() 
                      if time.time() - data['stats']['last_active'] < 86400)
    total_tokens = sum(u['stats'].get('total_tokens', 0) for u in user_data.values())
    total_models = len(model_info_cache)
    
    tested_models = [m for m, info in model_info_cache.items() if info.get('benchmark_tested', False)]
    top_benchmark_models = sorted(
        [(m, model_info_cache[m]) for m in tested_models],
        key=lambda x: x[1].get('benchmark_score', 0),
        reverse=True
    )[:3]
    
    benchmark_info = ""
    if top_benchmark_models:
        benchmark_info = "\n*üèÜ –¢–æ–ø –º–æ–¥–µ–ª–µ–π –ø–æ –±–µ–Ω—á–º–∞—Ä–∫—É:*\n"
        for model, info in top_benchmark_models:
            score = info.get('benchmark_score', 0)
            runs = info.get('benchmark_runs', 0)
            benchmark_info += f"‚Ä¢ {model}: {score:.2f}/1.0 ({runs} —Ç–µ—Å—Ç–æ–≤)\n"
    
    admin_text = (
        "üõ† *–ê–¥–º–∏–Ω-–ø–∞–Ω–µ–ª—å*\n\n"
        f"‚Ä¢ –í—Å–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {total_users}\n"
        f"‚Ä¢ –ê–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞ —Å—É—Ç–∫–∏: {active_users}\n"
        f"‚Ä¢ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {total_models}\n"
        f"‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ —Ç–æ–∫–µ–Ω–æ–≤: {total_tokens}\n"
        f"‚Ä¢ –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(tested_models)}\n"
        f"{benchmark_info}"
    )
    
    keyboard = [
        [InlineKeyboardButton("üèÜ –ë–µ–Ω—á–º–∞—Ä–∫ –º–æ–¥–µ–ª–µ–π", callback_data='benchmark_menu')],
        [InlineKeyboardButton("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞", callback_data='benchmark_stats')],
        [InlineKeyboardButton("üîÑ –û–±–Ω–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ", callback_data='admin_panel')],
        [InlineKeyboardButton("üíæ –°–æ–∑–¥–∞—Ç—å –±—ç–∫–∞–ø", callback_data='force_backup')],
        [InlineKeyboardButton("üîô –ù–∞–∑–∞–¥", callback_data='back_to_menu')]
    ]
    
    await query.edit_message_text(
        admin_text,
        reply_markup=InlineKeyboardMarkup(keyboard),
        parse_mode='Markdown'
    )

# –ù–æ–≤—ã–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∞
@handle_errors
async def benchmark_menu_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –º–µ–Ω—é –±–µ–Ω—á–º–∞—Ä–∫–∞"""
    query = update.callback_query
    user_id = query.from_user.id
    
    await query.answer()
    initialize_user(user_id)
    
    await show_benchmark_menu(query)

async def show_benchmark_menu(query, text: str = None):
    """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –º–µ–Ω—é –±–µ–Ω—á–º–∞—Ä–∫–∞"""
    user_id = query.from_user.id
    
    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    tested_models = len([m for m, info in model_info_cache.items() if info.get('benchmark_tested', False)])
    total_benchmarks = sum(u['stats']['benchmarks_run'] for u in user_data.values())
    
    menu_text = text or (
        f"üèÜ *–ú–µ–Ω—é –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–∞*\n\n"
        f"–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ –º–æ–¥–µ–ª–µ–π: {tested_models}\n"
        f"–í—Å–µ–≥–æ –∑–∞–ø—É—Å–∫–æ–≤: {total_benchmarks}\n\n"
        f"–ó–¥–µ—Å—å –≤—ã –º–æ–∂–µ—Ç–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."
    )
    
    keyboard = [
        [InlineKeyboardButton("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫", callback_data='run_full_benchmark')],
        [InlineKeyboardButton("üìä –ü–æ–∫–∞–∑–∞—Ç—å –ª–∏–¥–µ—Ä–±–æ—Ä–¥", callback_data='show_leaderboard')],
        [InlineKeyboardButton("üìã –°–ø–∏—Å–æ–∫ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–¥–∞–Ω–∏–π", callback_data='show_benchmark_tasks')],
        [InlineKeyboardButton("üß™ –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –æ–¥–Ω—É –º–æ–¥–µ–ª—å", callback_data='test_single_model')],
        [InlineKeyboardButton("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞", callback_data='benchmark_stats')],
        [InlineKeyboardButton("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à –±–µ–Ω—á–º–∞—Ä–∫–∞", callback_data='clear_benchmark_cache')],
        [InlineKeyboardButton("üîô –ù–∞–∑–∞–¥", callback_data='admin_panel')]
    ]
    
    await query.edit_message_text(
        menu_text,
        reply_markup=InlineKeyboardMarkup(keyboard),
        parse_mode='Markdown'
    )

@handle_errors
async def run_full_benchmark_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∑–∞–ø—É—Å–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞"""
    query = update.callback_query
    user_id = query.from_user.id
    
    await query.answer()
    initialize_user(user_id)
    
    if user_data[user_id]['benchmark_status'] == BenchmarkStatus.RUNNING.value:
        await query.answer("‚ö†Ô∏è –ë–µ–Ω—á–º–∞—Ä–∫ —É–∂–µ –∑–∞–ø—É—â–µ–Ω", show_alert=True)
        return
    
    models = await get_available_models(refresh=True)
    if not models:
        await query.edit_message_text(
            "‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.",
            reply_markup=InlineKeyboardMarkup([
                [InlineKeyboardButton("üîô –ù–∞–∑–∞–¥", callback_data='benchmark_menu')]
            ])
        )
        return
    
    total_tasks = len(models) * len(CONFIG['BENCHMARK_TASKS'])
    estimated_time = total_tasks * 20  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ 20 —Å–µ–∫—É–Ω–¥ –Ω–∞ –∑–∞–¥–∞—á—É
    
    await query.edit_message_text(
        f"üöÄ *–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞*\n\n"
        f"–ë—É–¥—É—Ç –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã {len(models)} –º–æ–¥–µ–ª–µ–π:\n"
        + "\n".join(f"‚Ä¢ {model}" for model in models[:10]) +
        (f"\n... –∏ –µ—â–µ {len(models) - 10} –º–æ–¥–µ–ª–µ–π" if len(models) > 10 else "") +
        f"\n\n–í—Å–µ–≥–æ –∑–∞–¥–∞–Ω–∏–π: {total_tasks}"
        f"\n–û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è: ~{estimated_time//60} –º–∏–Ω—É—Ç {estimated_time%60} —Å–µ–∫—É–Ω–¥"
        f"\n\n*–ù–∞—á–∏–Ω–∞—é —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ...*",
        parse_mode='Markdown'
    )
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫ –≤ —Ñ–æ–Ω–µ
    asyncio.create_task(run_benchmark_and_report(models, user_id, query, context))

async def run_benchmark_and_report(models: List[str], user_id: int, query, context: ContextTypes.DEFAULT_TYPE):
    """–ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ –æ—Ç—á–µ—Ç–æ–≤"""
    try:
        results = await run_full_benchmark(models, user_id, query)
        
        if results:
            report = format_benchmark_results(results)
            
            timestamp = int(time.time())
            report_filename = f'benchmarks/report_{user_id}_{timestamp}.txt'
            with open(report_filename, 'w', encoding='utf-8') as f:
                f.write(report)
            
            if len(report) > 4000:
                parts = split_long_message(report, 4000)
                for i, part in enumerate(parts):
                    if i == 0:
                        await query.edit_message_text(
                            f"üìä *–û—Ç—á–µ—Ç –æ –±–µ–Ω—á–º–∞—Ä–∫–µ* (—á–∞—Å—Ç—å {i+1}/{len(parts)})\n\n{part}",
                            parse_mode='Markdown'
                        )
                    else:
                        await context.bot.send_message(
                            chat_id=query.message.chat_id,
                            text=f"üìä *–û—Ç—á–µ—Ç –æ –±–µ–Ω—á–º–∞—Ä–∫–µ* (—á–∞—Å—Ç—å {i+1}/{len(parts)})\n\n{part}",
                            parse_mode='Markdown'
                        )
            else:
                await query.edit_message_text(
                    f"üìä *–û—Ç—á–µ—Ç –æ –±–µ–Ω—á–º–∞—Ä–∫–µ*\n\n{report}",
                    parse_mode='Markdown',
                    reply_markup=InlineKeyboardMarkup([
                        [InlineKeyboardButton("üèÜ –õ–∏–¥–µ—Ä–±–æ—Ä–¥", callback_data='show_leaderboard')],
                        [InlineKeyboardButton("üîô –í –º–µ–Ω—é", callback_data='benchmark_menu')]
                    ])
                )
        else:
            await query.edit_message_text(
                "‚ùå –ë–µ–Ω—á–º–∞—Ä–∫ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –±–µ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. –í–æ–∑–º–æ–∂–Ω–æ, –≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã.",
                reply_markup=InlineKeyboardMarkup([
                    [InlineKeyboardButton("üîô –í –º–µ–Ω—é", callback_data='benchmark_menu')]
                ])
            )
            
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞: {e}", exc_info=True)
        await query.edit_message_text(
            f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞: {str(e)}",
            reply_markup=InlineKeyboardMarkup([
                [InlineKeyboardButton("üîô –í –º–µ–Ω—é", callback_data='benchmark_menu')]
            ])
        )

@handle_errors
async def show_leaderboard_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –ª–∏–¥–µ—Ä–±–æ—Ä–¥–∞"""
    query = update.callback_query
    user_id = query.from_user.id
    
    await query.answer()
    initialize_user(user_id)
    
    # –ü–æ–ª—É—á–∞–µ–º –ª–∏–¥–µ—Ä–±–æ—Ä–¥
    leaderboard = await get_model_leaderboard()
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
    current_time = datetime.now().strftime('%H:%M:%S')
    leaderboard_text = f"{leaderboard}\n\n_‚è± –û–±–Ω–æ–≤–ª–µ–Ω–æ –≤ {current_time}_"
    
    await query.edit_message_text(
        leaderboard_text,
        parse_mode='Markdown',
        reply_markup=InlineKeyboardMarkup([
            [InlineKeyboardButton("üîÑ –û–±–Ω–æ–≤–∏—Ç—å", callback_data='show_leaderboard')],
            [InlineKeyboardButton("üîô –í –º–µ–Ω—é", callback_data='benchmark_menu')]
        ])
    )

@handle_errors
async def show_benchmark_tasks_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–¥–∞–Ω–∏–π"""
    query = update.callback_query
    user_id = query.from_user.id
    
    await query.answer()
    initialize_user(user_id)
    
    tasks_text = "üìã *–¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–¥–∞–Ω–∏—è –±–µ–Ω—á–º–∞—Ä–∫–∞*\n\n"
    for i, task in enumerate(CONFIG['BENCHMARK_TASKS']):
        weight = task.get('weight', 1.0)
        weight_text = f" ‚öñÔ∏è{weight}" if weight != 1.0 else ""
        timeout = task.get('timeout', CONFIG['BENCHMARK_DEFAULT_TIMEOUT'])
        
        tasks_text += f"*{i+1}. {task['name']}{weight_text}*\n"
        tasks_text += f"   –¢–∞–π–º–∞—É—Ç: {timeout}—Å | –¢–æ–∫–µ–Ω—ã: {task.get('max_tokens', CONFIG['BENCHMARK_MAX_TOKENS'])}\n"
        tasks_text += f"   –ó–∞–ø—Ä–æ—Å: `{task['prompt']}`\n"
        tasks_text += f"   –û–∂–∏–¥–∞–µ–º—ã–π –æ—Ç–≤–µ—Ç: {task['expected_answer']}\n\n"
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã—Ö —Ç–∞–π–º–∞—É—Ç–æ–≤
    task_stats = adaptive_benchmark.get_task_stats()
    if task_stats:
        tasks_text += "*üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:*\n"
        for task_id, stats in task_stats.items():
            task_name = next((t['name'] for t in CONFIG['BENCHMARK_TASKS'] if t['id'] == task_id), task_id)
            tasks_text += f"  ‚Ä¢ {task_name}: {stats['count']} —Ç–µ—Å—Ç–æ–≤, —Å—Ä. {stats['avg_time']:.1f}—Å\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º—è –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
    current_time = datetime.now().strftime('%H:%M:%S')
    tasks_text_with_time = f"{tasks_text}\n\n_‚è± –ü—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–æ –≤ {current_time}_"
    
    await query.edit_message_text(
        tasks_text_with_time,
        parse_mode='Markdown',
        reply_markup=InlineKeyboardMarkup([
            [InlineKeyboardButton("üîô –í –º–µ–Ω—é", callback_data='benchmark_menu')]
        ])
    )

@handle_errors
async def test_single_model_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    query = update.callback_query
    user_id = query.from_user.id
    
    await query.answer()
    initialize_user(user_id)
    
    models = await get_available_models()
    if not models:
        await query.edit_message_text(
            "‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.",
            reply_markup=InlineKeyboardMarkup([
                [InlineKeyboardButton("üîô –í –º–µ–Ω—é", callback_data='benchmark_menu')]
            ])
        )
        return
    
    buttons = []
    for model in models[:20]:
        model_info = model_info_cache.get(model, {})
        score = model_info.get('benchmark_score', 0)
        runs = model_info.get('benchmark_runs', 0)
        
        score_text = f" ‚≠ê{score:.1f}" if score > 0 else ""
        runs_text = f" ({runs} —Ç–µ—Å—Ç–æ–≤)" if runs > 0 else ""
        
        buttons.append(
            InlineKeyboardButton(f"{model}{score_text}{runs_text}", 
                               callback_data=f"benchmark_model_{model}")
        )
    
    keyboard = [buttons[i:i + 2] for i in range(0, len(buttons), 2)]
    keyboard.append([InlineKeyboardButton("üîô –í –º–µ–Ω—é", callback_data='benchmark_menu')])
    
    await query.edit_message_text(
        "üß™ *–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏*\n\n–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:",
        reply_markup=InlineKeyboardMarkup(keyboard),
        parse_mode='Markdown'
    )

@handle_errors
async def benchmark_model_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    query = update.callback_query
    user_id = query.from_user.id
    
    await query.answer()
    initialize_user(user_id)
    
    model_name = query.data.replace('benchmark_model_', '')
    
    await query.edit_message_text(
        f"üß™ *–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_name}*\n\n–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é...",
        parse_mode='Markdown'
    )
    
    # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏
    tasks = []
    for task_config in CONFIG['BENCHMARK_TASKS']:
        task = BenchmarkTask(
            id=task_config['id'],
            name=task_config['name'],
            prompt=task_config['prompt'],
            expected_answer=task_config['expected_answer'],
            timeout=task_config.get('timeout', CONFIG['BENCHMARK_DEFAULT_TIMEOUT']),
            max_tokens=task_config.get('max_tokens', CONFIG['BENCHMARK_MAX_TOKENS']),
            weight=task_config.get('weight', 1.0)
        )
        tasks.append(task)
    
    total_tasks = len(tasks)
    progress_tracker = BenchmarkProgressTracker(query, total_tasks)
    await progress_tracker.update_display()
    
    results = []
    
    for task in tasks:
        progress_tracker.increment(model_name, task.name)
        
        result = await run_benchmark_task(model_name, task)
        if result:
            results.append(result)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ benchmark_results
            if model_name not in benchmark_results:
                benchmark_results[model_name] = []
            benchmark_results[model_name].append(result.to_dict())
        
        await progress_tracker.update_display()
    
    await progress_tracker.final_update()
    
    if results:
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞
        total_weight = sum(r.task_weight for r in results)
        weighted_sum = sum(r.score * r.task_weight for r in results)
        avg_score = weighted_sum / total_weight if total_weight > 0 else 0
        
        avg_time = statistics.mean([r.response_time for r in results])
        avg_tps = statistics.mean([r.tokens_per_second for r in results])
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏
        if model_name in model_info_cache:
            model_info_cache[model_name]['benchmark_score'] = avg_score
            model_info_cache[model_name]['benchmark_tested'] = True
            model_info_cache[model_name]['benchmark_runs'] = model_info_cache[model_name].get('benchmark_runs', 0) + 1
            model_info_cache[model_name]['last_benchmark'] = time.time()
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report = f"üìä *–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {model_name}*\n\n"
        report += f"–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞: *{avg_score:.2f}/1.0*\n"
        report += f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: *{avg_time:.2f}—Å*\n"
        report += f"–°–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: *{avg_tps:.1f} —Ç–æ–∫–µ–Ω–æ–≤/—Å–µ–∫*\n\n"
        report += "*–î–µ—Ç–∞–ª–∏ –ø–æ –∑–∞–¥–∞—á–∞–º:*\n"
        
        for result in results:
            status = "‚úÖ" if result.score >= 0.7 else "‚ö†Ô∏è" if result.score >= 0.3 else "‚ùå"
            weight_marker = f" ‚öñÔ∏è{result.task_weight}" if result.task_weight != 1.0 else ""
            timeout_info = f" (—Ç–∞–π–º–∞—É—Ç: {result.timeout_used}—Å)"
            
            report += f"\n{status}{weight_marker} *{result.task_name}:* {result.score:.1f}{timeout_info}\n"
            report += f"   –í—Ä–µ–º—è: {result.response_time:.1f}—Å\n"
            report += f"   –í–µ—Ä–¥–∏–∫—Ç: {result.evaluation}\n"
            report += f"   –û—Ç–≤–µ—Ç: _{result.raw_response}_\n"
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        await save_benchmark_results({model_name: {
            'avg_score': avg_score,
            'weighted_score': avg_score,
            'avg_time': avg_time,
            'avg_tps': avg_tps,
            'results': results
        }}, user_id)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –ª–∏–¥–µ—Ä–æ–≤ —Å —ç—Ç–æ–π –º–æ–¥–µ–ª—å—é
        leaderboard = await get_model_leaderboard()
        leaderboard_lines = leaderboard.split('\n')
        short_leaderboard = '\n'.join(leaderboard_lines[:20])  # –ü–µ—Ä–≤—ã–µ 20 —Å—Ç—Ä–æ–∫
        
        report += f"\n\nüèÜ *–¢–∞–±–ª–∏—Ü–∞ –ª–∏–¥–µ—Ä–æ–≤ (—Ñ—Ä–∞–≥–º–µ–Ω—Ç):*\n{short_leaderboard}"
        
        await query.edit_message_text(
            report,
            parse_mode='Markdown',
            reply_markup=InlineKeyboardMarkup([
                [InlineKeyboardButton("üèÜ –ü–æ–ª–Ω—ã–π –ª–∏–¥–µ—Ä–±–æ—Ä–¥", callback_data='show_leaderboard')],
                [InlineKeyboardButton("üîô –í –º–µ–Ω—é", callback_data='benchmark_menu')]
            ])
        )
    else:
        await query.edit_message_text(
            f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å {model_name}",
            reply_markup=InlineKeyboardMarkup([
                [InlineKeyboardButton("üîô –í –º–µ–Ω—é", callback_data='benchmark_menu')]
            ])
        )

@handle_errors
async def benchmark_stats_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞"""
    query = update.callback_query
    user_id = query.from_user.id
    
    await query.answer()
    initialize_user(user_id)
    
    total_benchmarks = sum(u['stats']['benchmarks_run'] for u in user_data.values())
    total_tested_models = len([m for m, info in model_info_cache.items() if info.get('benchmark_tested', False)])
    
    task_stats = adaptive_benchmark.get_task_stats()
    total_task_runs = sum(stats['count'] for stats in task_stats.values()) if task_stats else 0
    
    # –°–æ–±–∏—Ä–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    user_benchmarks = user_data[user_id].get('benchmark_history', [])
    user_benchmarks_text = ""
    if user_benchmarks:
        user_benchmarks_text = "\n*üìÖ –í–∞—à–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –±–µ–Ω—á–º–∞—Ä–∫–∏:*\n"
        for hist in user_benchmarks[-5:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5
            dt = datetime.fromtimestamp(hist['timestamp'])
            models = hist.get('models_tested', [])
            user_benchmarks_text += f"‚Ä¢ {dt.strftime('%d.%m %H:%M')}: {len(models)} –º–æ–¥–µ–ª–µ–π\n"
    
    stats_text = (
        f"üìà *–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞*\n\n"
        f"‚Ä¢ –í—Å–µ–≥–æ –∑–∞–ø—É—Å–∫–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–∞: {total_benchmarks}\n"
        f"‚Ä¢ –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ –º–æ–¥–µ–ª–µ–π: {total_tested_models}\n"
        f"‚Ä¢ –í—Å–µ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ –∑–∞–¥–∞—á: {total_task_runs}\n"
        f"‚Ä¢ –ê–∫—Ç–∏–≤–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤: {sum(1 for u in user_data.values() if u['benchmark_status'] == 'running')}\n"
        f"{user_benchmarks_text}"
    )
    
    if task_stats:
        stats_text += f"\n*üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–¥–∞—á–∞–º:*\n"
        for task_id, stats in task_stats.items():
            task_name = next((t['name'] for t in CONFIG['BENCHMARK_TASKS'] if t['id'] == task_id), task_id)
            current_timeout = adaptive_benchmark.task_timeouts.get(task_id, 'N/A')
            stats_text += f"‚Ä¢ {task_name}: {stats['count']} —Ç–µ—Å—Ç–æ–≤, —Å—Ä. {stats['avg_time']:.1f}—Å, —Ç–∞–π–º–∞—É—Ç {current_timeout}—Å\n"
    
    # –¢–æ–ø –º–æ–¥–µ–ª–µ–π
    tested_models = [(m, info) for m, info in model_info_cache.items() if info.get('benchmark_tested', False)]
    if tested_models:
        sorted_models = sorted(tested_models, key=lambda x: x[1].get('benchmark_score', 0), reverse=True)
        stats_text += f"\n*üèÜ –¢–æ–ø-5 –º–æ–¥–µ–ª–µ–π:*\n"
        for i, (model, info) in enumerate(sorted_models[:5]):
            score = info.get('benchmark_score', 0)
            runs = info.get('benchmark_runs', 0)
            stats_text += f"{i+1}. {model}: {score:.2f} ({runs} —Ç–µ—Å—Ç–æ–≤)\n"
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
    current_time = datetime.now().strftime('%H:%M:%S')
    stats_text_with_time = f"{stats_text}\n\n_‚è± –û–±–Ω–æ–≤–ª–µ–Ω–æ –≤ {current_time}_"
    
    await query.edit_message_text(
        stats_text_with_time,
        parse_mode='Markdown',
        reply_markup=InlineKeyboardMarkup([
            [InlineKeyboardButton("üîÑ –û–±–Ω–æ–≤–∏—Ç—å", callback_data='benchmark_stats')],
            [InlineKeyboardButton("üîô –í –º–µ–Ω—é", callback_data='benchmark_menu')]
        ])
    )

@handle_errors
async def clear_benchmark_cache_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞"""
    query = update.callback_query
    user_id = query.from_user.id
    
    await query.answer()
    initialize_user(user_id)
    
    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Ç–∞–π–º–∞—É—Ç—ã
    adaptive_benchmark.task_times.clear()
    adaptive_benchmark.task_timeouts.clear()
    
    # –û—á–∏—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞ (–Ω–æ –Ω–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö)
    benchmark_results.clear()
    
    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è, –Ω–æ –æ—Å—Ç–∞–≤–ª—è–µ–º –æ—Ü–µ–Ω–∫–∏
    for model in model_info_cache:
        if 'benchmark_tested' in model_info_cache[model]:
            model_info_cache[model]['benchmark_tested'] = False
            model_info_cache[model]['benchmark_runs'] = 0
    
    await query.edit_message_text(
        "‚úÖ –ö—ç—à –±–µ–Ω—á–º–∞—Ä–∫–∞ –æ—á–∏—â–µ–Ω. –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ —Ç–∞–π–º–∞—É—Ç—ã —Å–±—Ä–æ—à–µ–Ω—ã.\n\n"
        "–û—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã, –Ω–æ —Å—Ç–∞—Ç—É—Å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–±—Ä–æ—à–µ–Ω.",
        reply_markup=InlineKeyboardMarkup([
            [InlineKeyboardButton("üîô –í –º–µ–Ω—é", callback_data='benchmark_menu')]
        ])
    )

async def post_init(application):
    """–ü–æ—Å—Ç-–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è"""
    logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    if application.job_queue:
        application.job_queue.run_repeating(
            backup_task,
            interval=CONFIG['BACKUP_INTERVAL'],
            first=10
        )

async def shutdown():
    """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã"""
    logger.info("–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
    shutdown_event.set()
    
    for task in active_requests.values():
        task.cancel()
        try:
            await task
        except asyncio.CancelledError:
            pass
    
    await backup_data()
    logger.info("–§–∏–Ω–∞–ª—å–Ω—ã–π –±—ç–∫–∞–ø —Å–æ–∑–¥–∞–Ω")
    
    application = ApplicationBuilder().build()
    await application.stop()
    await application.shutdown()

def handle_signal(signum, frame):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è"""
    logger.info(f"–ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª {signum}, –∑–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞–±–æ—Ç—É...")
    asyncio.create_task(shutdown())

async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    setup_logging()
    setup_directories()
    
    try:
        await load_backup()
        
        application = ApplicationBuilder() \
            .token(CONFIG['TOKEN']) \
            .post_init(post_init) \
            .build()
        
        # –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∫–æ–º–∞–Ω–¥
        application.add_handler(CommandHandler('start', start))
        application.add_handler(CommandHandler('help', start))
        application.add_handler(CommandHandler('model', lambda u, c: show_models_menu(u.callback_query)))
        application.add_handler(CommandHandler('stats', lambda u, c: show_stats_menu(u.callback_query)))
        application.add_handler(CommandHandler('settings', lambda u, c: show_settings_menu(u.callback_query)))
        application.add_handler(CommandHandler('benchmark', benchmark_command))
        application.add_handler(CommandHandler('leaderboard', leaderboard_command))
        application.add_handler(CommandHandler('benchmark_stats', stats_command))
        
        # –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
        application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
        application.add_handler(CallbackQueryHandler(button_handler))
        
        logger.info("–ë–æ—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è...")
        await application.initialize()
        await application.start()
        await application.updater.start_polling()
        
        while True:
            await asyncio.sleep(1)
            
    except asyncio.CancelledError:
        pass
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±–æ—Ç–∞: {e}", exc_info=True)
    finally:
        await shutdown()
        logger.info("–ë–æ—Ç –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É")

if __name__ == '__main__':
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass
